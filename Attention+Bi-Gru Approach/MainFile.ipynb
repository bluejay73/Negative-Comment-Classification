{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import codecs\n",
    "from tqdm import tqdm\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Conv1D, Conv2D, MaxPooling1D, GlobalMaxPool1D, Bidirectional, GlobalMaxPooling1D\n",
    "from keras.layers import LSTM, GRU, Dropout , BatchNormalization, Embedding, Flatten, GlobalAveragePooling1D, concatenate, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "train.dropna(inplace=True)\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(train, random_state=42, test_size=0.30, shuffle=True)\n",
    "\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "#list_classes =list_classes[0:100000]\n",
    "\n",
    "y_train = train[list_classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = test[list_classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train[list_classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111699"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train[list_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47872"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test[list_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>111699.000000</td>\n",
       "      <td>111699.000000</td>\n",
       "      <td>111699.000000</td>\n",
       "      <td>111699.000000</td>\n",
       "      <td>111699.000000</td>\n",
       "      <td>111699.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.095901</td>\n",
       "      <td>0.009928</td>\n",
       "      <td>0.052758</td>\n",
       "      <td>0.003062</td>\n",
       "      <td>0.049132</td>\n",
       "      <td>0.008711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.294456</td>\n",
       "      <td>0.099146</td>\n",
       "      <td>0.223551</td>\n",
       "      <td>0.055249</td>\n",
       "      <td>0.216145</td>\n",
       "      <td>0.092925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               toxic   severe_toxic        obscene         threat  \\\n",
       "count  111699.000000  111699.000000  111699.000000  111699.000000   \n",
       "mean        0.095901       0.009928       0.052758       0.003062   \n",
       "std         0.294456       0.099146       0.223551       0.055249   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         0.000000       0.000000       0.000000       0.000000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "              insult  identity_hate  \n",
       "count  111699.000000  111699.000000  \n",
       "mean        0.049132       0.008711  \n",
       "std         0.216145       0.092925  \n",
       "min         0.000000       0.000000  \n",
       "25%         0.000000       0.000000  \n",
       "50%         0.000000       0.000000  \n",
       "75%         0.000000       0.000000  \n",
       "max         1.000000       1.000000  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test = pd.read_csv('test.csv')\n",
    "#test.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>47872.000000</td>\n",
       "      <td>47872.000000</td>\n",
       "      <td>47872.000000</td>\n",
       "      <td>47872.000000</td>\n",
       "      <td>47872.000000</td>\n",
       "      <td>47872.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.095714</td>\n",
       "      <td>0.010152</td>\n",
       "      <td>0.053392</td>\n",
       "      <td>0.002841</td>\n",
       "      <td>0.049904</td>\n",
       "      <td>0.009024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.294201</td>\n",
       "      <td>0.100246</td>\n",
       "      <td>0.224817</td>\n",
       "      <td>0.053225</td>\n",
       "      <td>0.217749</td>\n",
       "      <td>0.094566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              toxic  severe_toxic       obscene        threat        insult  \\\n",
       "count  47872.000000  47872.000000  47872.000000  47872.000000  47872.000000   \n",
       "mean       0.095714      0.010152      0.053392      0.002841      0.049904   \n",
       "std        0.294201      0.100246      0.224817      0.053225      0.217749   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "       identity_hate  \n",
       "count   47872.000000  \n",
       "mean        0.009024  \n",
       "std         0.094566  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         0.000000  \n",
       "75%         0.000000  \n",
       "max         1.000000  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id               False\n",
       "comment_text     False\n",
       "toxic            False\n",
       "severe_toxic     False\n",
       "obscene          False\n",
       "threat           False\n",
       "insult           False\n",
       "identity_hate    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id               False\n",
       "comment_text     False\n",
       "toxic            False\n",
       "severe_toxic     False\n",
       "obscene          False\n",
       "threat           False\n",
       "insult           False\n",
       "identity_hate    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update(['.', ',', '\"', \"'\", ':', ';', '(', ')', '[', ']', '{', '}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1856/111699 [00:00<00:05, 18556.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\n",
      "\n",
      "Conflict of interest note\n",
      "By your user name, it appears that you represent a company or organization. Please read our conflict of interest guidelines as well as our FAQ for businesses. We welcome your contributions here, but please refrain from writing about your own company's services and personnel. Thanks, and happy editing! ''''''Speaketh \"\n",
      "pre-processing train data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 111699/111699 [00:06<00:00, 17644.01it/s]\n",
      "  3%|▎         | 1541/47872 [00:00<00:03, 15409.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-processing test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47872/47872 [00:02<00:00, 16661.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing input data...\n",
      "('dictionary size: ', 187702)\n",
      "Done !!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "MAX_NB_WORDS = 100000\n",
    "max_seq_len = 50\n",
    "\n",
    "raw_docs_train = train['comment_text'].tolist()\n",
    "raw_docs_test = test['comment_text'].tolist()\n",
    "num_classes = len(list_classes)\n",
    "print raw_docs_train[0]\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "print(\"pre-processing train data...\")\n",
    "processed_docs_train = []\n",
    "for doc in tqdm(raw_docs_train):\n",
    "    tokens = tokenizer.tokenize(doc)\n",
    "    filtered = [word for word in tokens if word not in stop_words]\n",
    "    processed_docs_train.append(\" \".join(filtered))\n",
    "\n",
    "print(\"pre-processing test data...\")\n",
    "processed_docs_test = []\n",
    "for doc in tqdm(raw_docs_test):\n",
    "    tokens = tokenizer.tokenize(doc)\n",
    "    filtered = [word for word in tokens if word not in stop_words]\n",
    "    processed_docs_test.append(\" \".join(filtered))\n",
    "\n",
    "\n",
    "print(\"tokenizing input data...\")\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(processed_docs_train + processed_docs_test)  #leaky\n",
    "word_seq_train = tokenizer.texts_to_sequences(processed_docs_train)\n",
    "word_seq_test = tokenizer.texts_to_sequences(processed_docs_test)\n",
    "word_index = tokenizer.word_index\n",
    "print(\"dictionary size: \", len(word_index))\n",
    "\n",
    "#pad sequences\n",
    "word_seq_train = sequence.pad_sequences(word_seq_train, maxlen=max_seq_len)\n",
    "word_seq_test = sequence.pad_sequences(word_seq_test, maxlen=max_seq_len)\n",
    "\n",
    "print(\"Done !!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111699"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_seq_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "187702"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading first word embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [02:04, 3217.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 400000 word vectors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 300\n",
    "\n",
    "#load embeddings\n",
    "print('loading first word embeddings...')\n",
    "embeddings_index = {}\n",
    "f = codecs.open('EMB/glove.6B.300d.txt', encoding='utf-8')\n",
    "\n",
    "for line in tqdm(f):\n",
    "    values = line.rstrip().rsplit(' ')\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('found %s word vectors' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing embedding matrix...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sinchan/.local/lib/python2.7/site-packages/ipykernel_launcher.py:8: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of null word embeddings: 31473\n"
     ]
    }
   ],
   "source": [
    "print('preparing embedding matrix...')\n",
    "words_not_found = []\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix_glove = np.zeros((nb_words, embed_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i >= nb_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix_glove[i] = embedding_vector\n",
    "    else:\n",
    "        words_not_found.append(word)\n",
    "print('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix_glove, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/sinchan/.local/lib/python2.7/site-packages/IPython/core/ultratb.py\", line 1132, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/sinchan/.local/lib/python2.7/site-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/sinchan/.local/lib/python2.7/site-packages/IPython/core/ultratb.py\", line 358, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/usr/lib/python2.7/inspect.py\", line 1051, in getinnerframes\n",
      "    framelist.append((tb.tb_frame,) + getframeinfo(tb, context))\n",
      "  File \"/usr/lib/python2.7/inspect.py\", line 1011, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/usr/lib/python2.7/inspect.py\", line 453, in getsourcefile\n",
      "    if hasattr(getmodule(object, filename), '__loader__'):\n",
      "  File \"/usr/lib/python2.7/inspect.py\", line 496, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"/usr/lib/python2.7/inspect.py\", line 465, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"/usr/lib/python2.7/inspect.py\", line 450, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"/usr/lib/python2.7/genericpath.py\", line 26, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/home/sinchan/.local/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_code\u001b[0;34m(self, code_obj, result)\u001b[0m\n\u001b[1;32m   2893\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2894\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_in_exec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2895\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowtraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2896\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2897\u001b[0m             \u001b[0moutflag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sinchan/.local/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only)\u001b[0m\n\u001b[1;32m   1824\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0;32m-> 1826\u001b[0;31m                                             value, tb, tb_offset=tb_offset)\n\u001b[0m\u001b[1;32m   1827\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1828\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_showtraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sinchan/.local/lib/python2.7/site-packages/IPython/core/ultratb.pyc\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1409\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1410\u001b[0m         return FormattedTB.structured_traceback(\n\u001b[0;32m-> 1411\u001b[0;31m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0m\u001b[1;32m   1412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sinchan/.local/lib/python2.7/site-packages/IPython/core/ultratb.pyc\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1317\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m             return VerboseTB.structured_traceback(\n\u001b[0;32m-> 1319\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1320\u001b[0m             )\n\u001b[1;32m   1321\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sinchan/.local/lib/python2.7/site-packages/IPython/core/ultratb.pyc\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1202\u001b[0m                 \u001b[0mstructured_traceback_parts\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mformatted_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1203\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1204\u001b[0;31m             \u001b[0mstructured_traceback_parts\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mformatted_exception\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1206\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstructured_traceback_parts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "w2v = KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin\",binary = True,limit=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'w2v' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-4eefdc671fcc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0membedding_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw2v\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mx_w\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'w2v' is not defined"
     ]
    }
   ],
   "source": [
    "nb_words = len(word_index)\n",
    "EMBEDDING_DIM =300\n",
    "embedding_matrix = np.zeros((nb_words+1, EMBEDDING_DIM))\n",
    "\n",
    "i = 0\n",
    "x_g = x_w = 0\n",
    "for word in word_index:   \n",
    "    try:\n",
    "        embedding_matrix[word_index[word]][:300] = w2v[word]\n",
    "    except KeyError:\n",
    "        x_w += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding_matrix_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(max_seq_len, ))\n",
    "emb_glove = Embedding(len(embedding_matrix_glove), 300, \n",
    "                weights=[embedding_matrix_glove], input_length = max_seq_len, trainable=False)(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Bidirectional(LSTM(25,return_sequences=True))(emb_glove)\n",
    "x = Bidirectional(GRU (25, return_sequences=True))(x)\n",
    "avg_pool = GlobalAveragePooling1D()(x)\n",
    "max_pool = GlobalMaxPooling1D()(x)\n",
    "out = concatenate([avg_pool, max_pool])\n",
    "\n",
    "out = Dense(10, activation=\"relu\")(out)\n",
    "out = Dense(y_train.shape[1], activation=\"sigmoid\")(out)\n",
    "\n",
    "model2 = Model(inputs=inp, outputs=out)\n",
    "\n",
    "model2.compile(loss='binary_crossentropy', optimizer = keras.optimizers.Adam(lr=0.001),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test = pd.read_csv('test_labels.csv')\n",
    "y_test = Y_test[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values\n",
    "y_test=y_test[0:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/50\n",
      " - 405s - loss: 0.0721 - acc: 0.9769 - val_loss: -2.7931e+00 - val_acc: 0.4065\n",
      "Epoch 2/50\n",
      " - 401s - loss: 0.0507 - acc: 0.9815 - val_loss: -3.1115e+00 - val_acc: 0.4033\n",
      "Epoch 3/50\n",
      " - 401s - loss: 0.0472 - acc: 0.9825 - val_loss: -3.3943e+00 - val_acc: 0.4055\n",
      "Epoch 4/50\n",
      " - 404s - loss: 0.0442 - acc: 0.9834 - val_loss: -3.2545e+00 - val_acc: 0.4050\n",
      "Epoch 5/50\n",
      " - 401s - loss: 0.0413 - acc: 0.9843 - val_loss: -3.5338e+00 - val_acc: 0.4056\n",
      "Epoch 6/50\n",
      " - 401s - loss: 0.0386 - acc: 0.9853 - val_loss: -3.9460e+00 - val_acc: 0.4070\n",
      "Epoch 7/50\n",
      " - 401s - loss: 0.0359 - acc: 0.9862 - val_loss: -3.9251e+00 - val_acc: 0.4062\n",
      "Epoch 8/50\n",
      " - 402s - loss: 0.0335 - acc: 0.9871 - val_loss: -4.0131e+00 - val_acc: 0.4053\n",
      "Epoch 9/50\n",
      " - 401s - loss: 0.0311 - acc: 0.9879 - val_loss: -4.2224e+00 - val_acc: 0.4051\n",
      "Epoch 10/50\n",
      " - 401s - loss: 0.0291 - acc: 0.9888 - val_loss: -4.1975e+00 - val_acc: 0.4044\n",
      "Epoch 11/50\n",
      " - 401s - loss: 0.0271 - acc: 0.9894 - val_loss: -4.3910e+00 - val_acc: 0.4054\n",
      "Epoch 12/50\n",
      " - 401s - loss: 0.0251 - acc: 0.9903 - val_loss: -4.8410e+00 - val_acc: 0.4052\n",
      "Epoch 13/50\n",
      " - 401s - loss: 0.0233 - acc: 0.9910 - val_loss: -4.9956e+00 - val_acc: 0.4046\n",
      "Epoch 14/50\n",
      " - 401s - loss: 0.0216 - acc: 0.9917 - val_loss: -5.1917e+00 - val_acc: 0.4050\n",
      "Epoch 15/50\n",
      " - 401s - loss: 0.0203 - acc: 0.9923 - val_loss: -5.4881e+00 - val_acc: 0.4047\n",
      "Epoch 16/50\n",
      " - 400s - loss: 0.0190 - acc: 0.9928 - val_loss: -5.3169e+00 - val_acc: 0.4038\n",
      "Epoch 17/50\n",
      " - 401s - loss: 0.0179 - acc: 0.9932 - val_loss: -5.4800e+00 - val_acc: 0.4047\n",
      "Epoch 18/50\n",
      " - 401s - loss: 0.0166 - acc: 0.9937 - val_loss: -5.7099e+00 - val_acc: 0.4046\n",
      "Epoch 19/50\n",
      " - 401s - loss: 0.0155 - acc: 0.9942 - val_loss: -5.7637e+00 - val_acc: 0.4041\n",
      "Epoch 20/50\n",
      " - 401s - loss: 0.0146 - acc: 0.9946 - val_loss: -5.7447e+00 - val_acc: 0.4042\n",
      "Epoch 21/50\n",
      " - 401s - loss: 0.0138 - acc: 0.9949 - val_loss: -6.0629e+00 - val_acc: 0.4047\n",
      "Epoch 22/50\n",
      " - 401s - loss: 0.0130 - acc: 0.9952 - val_loss: -6.0349e+00 - val_acc: 0.4045\n",
      "Epoch 23/50\n",
      " - 401s - loss: 0.0122 - acc: 0.9955 - val_loss: -6.1395e+00 - val_acc: 0.4043\n",
      "Epoch 24/50\n",
      " - 401s - loss: 0.0114 - acc: 0.9959 - val_loss: -6.1251e+00 - val_acc: 0.4034\n",
      "Epoch 25/50\n",
      " - 401s - loss: 0.0108 - acc: 0.9960 - val_loss: -6.4127e+00 - val_acc: 0.4042\n",
      "Epoch 26/50\n",
      " - 401s - loss: 0.0101 - acc: 0.9964 - val_loss: -6.3448e+00 - val_acc: 0.4038\n",
      "Epoch 27/50\n",
      " - 401s - loss: 0.0097 - acc: 0.9965 - val_loss: -5.9495e+00 - val_acc: 0.4022\n",
      "Epoch 28/50\n",
      " - 401s - loss: 0.0094 - acc: 0.9966 - val_loss: -6.4078e+00 - val_acc: 0.4035\n",
      "Epoch 29/50\n",
      " - 401s - loss: 0.0089 - acc: 0.9967 - val_loss: -6.2065e+00 - val_acc: 0.4029\n",
      "Epoch 30/50\n",
      " - 401s - loss: 0.0086 - acc: 0.9969 - val_loss: -6.3904e+00 - val_acc: 0.4033\n",
      "Epoch 31/50\n",
      " - 401s - loss: 0.0079 - acc: 0.9971 - val_loss: -6.3336e+00 - val_acc: 0.4032\n",
      "Epoch 32/50\n",
      " - 401s - loss: 0.0078 - acc: 0.9972 - val_loss: -6.2693e+00 - val_acc: 0.4027\n",
      "Epoch 33/50\n",
      " - 401s - loss: 0.0077 - acc: 0.9972 - val_loss: -6.3518e+00 - val_acc: 0.4030\n",
      "Epoch 34/50\n",
      " - 401s - loss: 0.0072 - acc: 0.9974 - val_loss: -6.5446e+00 - val_acc: 0.4038\n",
      "Epoch 35/50\n",
      " - 401s - loss: 0.0067 - acc: 0.9976 - val_loss: -6.4291e+00 - val_acc: 0.4030\n",
      "Epoch 36/50\n",
      " - 401s - loss: 0.0066 - acc: 0.9976 - val_loss: -6.5051e+00 - val_acc: 0.4031\n",
      "Epoch 37/50\n",
      " - 401s - loss: 0.0066 - acc: 0.9976 - val_loss: -6.3578e+00 - val_acc: 0.4025\n",
      "Epoch 38/50\n",
      " - 401s - loss: 0.0061 - acc: 0.9978 - val_loss: -6.5416e+00 - val_acc: 0.4033\n",
      "Epoch 39/50\n",
      " - 401s - loss: 0.0061 - acc: 0.9979 - val_loss: -6.5730e+00 - val_acc: 0.4032\n",
      "Epoch 40/50\n",
      " - 401s - loss: 0.0061 - acc: 0.9979 - val_loss: -6.4914e+00 - val_acc: 0.4028\n",
      "Epoch 41/50\n",
      " - 401s - loss: 0.0055 - acc: 0.9981 - val_loss: -6.6342e+00 - val_acc: 0.4036\n",
      "Epoch 42/50\n",
      " - 499s - loss: 0.0058 - acc: 0.9980 - val_loss: -6.5254e+00 - val_acc: 0.4029\n",
      "Epoch 43/50\n",
      " - 401s - loss: 0.0054 - acc: 0.9982 - val_loss: -6.2887e+00 - val_acc: 0.4018\n",
      "Epoch 44/50\n",
      " - 402s - loss: 0.0055 - acc: 0.9981 - val_loss: -6.5388e+00 - val_acc: 0.4027\n",
      "Epoch 45/50\n",
      " - 401s - loss: 0.0050 - acc: 0.9983 - val_loss: -6.6929e+00 - val_acc: 0.4034\n",
      "Epoch 46/50\n",
      " - 401s - loss: 0.0050 - acc: 0.9983 - val_loss: -6.5640e+00 - val_acc: 0.4031\n",
      "Epoch 47/50\n",
      " - 401s - loss: 0.0048 - acc: 0.9983 - val_loss: -6.5200e+00 - val_acc: 0.4027\n",
      "Epoch 48/50\n",
      " - 401s - loss: 0.0047 - acc: 0.9984 - val_loss: -6.5503e+00 - val_acc: 0.4030\n",
      "Epoch 49/50\n",
      " - 401s - loss: 0.0046 - acc: 0.9984 - val_loss: -6.6211e+00 - val_acc: 0.4032\n",
      "Epoch 50/50\n",
      " - 401s - loss: 0.0047 - acc: 0.9984 - val_loss: -6.5800e+00 - val_acc: 0.4030\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f614fc42e50>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(word_seq_train, y_train, validation_data=(np.array(word_seq_test),np.array(y_test)),epochs=50, batch_size=80, shuffle=True,verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 111699 samples, validate on 47872 samples\n",
      "Epoch 1/10\n",
      " - 675s - loss: 0.0744 - acc: 0.9767 - val_loss: 0.0536 - val_acc: 0.9809\n",
      "Epoch 2/10\n",
      " - 786s - loss: 0.0504 - acc: 0.9817 - val_loss: 0.0501 - val_acc: 0.9819\n",
      "Epoch 3/10\n",
      " - 927s - loss: 0.0466 - acc: 0.9827 - val_loss: 0.0487 - val_acc: 0.9823\n",
      "Epoch 4/10\n"
     ]
    }
   ],
   "source": [
    "model2.fit(word_seq_train, y_train, validation_data=(np.array(word_seq_test),np.array(y_test)),epochs=10, batch_size=80, shuffle=True,verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model.save('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import backend as K\n",
    "import os\n",
    "\n",
    "def set_keras_backend(backend):\n",
    "\n",
    "    if K.backend() != backend:\n",
    "        os.environ['KERAS_BACKEND'] = backend\n",
    "        reload(K)\n",
    "        assert K.backend() == backend\n",
    "\n",
    "set_keras_backend(\"theano\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import initializers\n",
    "import numpy as np\n",
    "class AttLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.init = initializers.get('normal')\n",
    "        super(AttLayer, self).__init__(** kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape)==3\n",
    "        self.W = self.add_weight(name='kernel', \n",
    "                                  shape=(input_shape[-1],),\n",
    "                                  initializer='normal',\n",
    "                                  trainable=True)\n",
    "        super(AttLayer, self).build(input_shape) \n",
    "     \n",
    "\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        eij = K.tanh(K.dot(x, self.W))\n",
    "\n",
    "        ai = K.exp(eij)\n",
    "        weights = ai/K.sum(ai, axis=1).dimshuffle(0,'x')\n",
    "\n",
    "        weighted_input = x*weights.dimshuffle(0,1,'x')\n",
    "        return weighted_input.sum(axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        print input_shape[0]\n",
    "        return (input_shape[0], input_shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "hello\n",
      "hello\n",
      "None\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "inp = Input(shape=(max_seq_len, ))\n",
    "print \"hello\"\n",
    "emb_glove = Embedding(len(embedding_matrix_glove), 300, \n",
    "                weights=[embedding_matrix_glove], input_length = max_seq_len, trainable=False)(inp)\n",
    "print \"hello\"\n",
    "x = Bidirectional(LSTM(25,return_sequences=True))(emb_glove)\n",
    "l_gru = Bidirectional(GRU(25,return_sequences=True))(x)\n",
    "print \"hello\"\n",
    "l_att = AttLayer()(l_gru)\n",
    "out = Dense(10, activation=\"relu\")(l_att)\n",
    "print \"hello\"\n",
    "preds = Dense(6, activation='softmax')(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inp, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer = keras.optimizers.Adam(lr=0.001),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 111699 samples, validate on 47872 samples\n",
      "Epoch 1/10\n",
      " - 769s - loss: 0.2231 - acc: 0.9633 - val_loss: 0.2213 - val_acc: 0.9626\n",
      "Epoch 2/10\n",
      " - 680s - loss: 0.2197 - acc: 0.9641 - val_loss: 0.2201 - val_acc: 0.9644\n",
      "Epoch 3/10\n",
      " - 683s - loss: 0.2181 - acc: 0.9646 - val_loss: 0.2196 - val_acc: 0.9630\n",
      "Epoch 4/10\n",
      " - 684s - loss: 0.2170 - acc: 0.9653 - val_loss: 0.2196 - val_acc: 0.9630\n",
      "Epoch 5/10\n",
      " - 674s - loss: 0.2160 - acc: 0.9656 - val_loss: 0.2203 - val_acc: 0.9625\n",
      "Epoch 6/10\n",
      " - 676s - loss: 0.2150 - acc: 0.9661 - val_loss: 0.2203 - val_acc: 0.9633\n",
      "Epoch 7/10\n",
      " - 678s - loss: 0.2140 - acc: 0.9665 - val_loss: 0.2214 - val_acc: 0.9636\n",
      "Epoch 8/10\n",
      " - 675s - loss: 0.2131 - acc: 0.9670 - val_loss: 0.2228 - val_acc: 0.9646\n",
      "Epoch 9/10\n",
      " - 677s - loss: 0.2122 - acc: 0.9673 - val_loss: 0.2224 - val_acc: 0.9633\n",
      "Epoch 10/10\n",
      " - 677s - loss: 0.2112 - acc: 0.9678 - val_loss: 0.2232 - val_acc: 0.9639\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f780920f210>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(word_seq_train, y_train, validation_data=(np.array(word_seq_test),np.array(y_test)),epochs=10, batch_size=80, shuffle=True,verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_model2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save('my_model3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>toxic</th>\n",
       "      <td>47%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>severe_toxic</th>\n",
       "      <td>0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>obscene</th>\n",
       "      <td>37%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>threat</th>\n",
       "      <td>0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>insult</th>\n",
       "      <td>13%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>identity_hate</th>\n",
       "      <td>0%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0\n",
       "toxic          47%\n",
       "severe_toxic    0%\n",
       "obscene        37%\n",
       "threat          0%\n",
       "insult         13%\n",
       "identity_hate   0%"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(processed_docs_train + processed_docs_test) \n",
    "s=\"what the fuck\"\n",
    "arr=[]\n",
    "arr.append(s)\n",
    "s = tokenizer.texts_to_sequences(arr)\n",
    "s = sequence.pad_sequences(s, maxlen=max_seq_len)\n",
    "res=model.predict(s)\n",
    "final=[]\n",
    "for i in range(len(res)):\n",
    "    for j in range(len(res[i])):\n",
    "        final.append(str(int(res[i][j]*100))+\"%\")\n",
    "l=[[\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]]\n",
    "pandas.DataFrame(final, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
